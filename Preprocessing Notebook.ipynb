{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Course Project Jupyter Notebook\n",
    "\n",
    "#### Data Files \n",
    "- business.csv \n",
    "- sample_submission.csv\n",
    "- test_queries.csv\n",
    "- train_reviews.csv\n",
    "- user.csv\n",
    "- validate_queries.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Business Data\n",
    "\n",
    "Expects the csv file to be in an \"all\" folder in the working directory of this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_df = pd.read_csv(\"all/business.csv\", engine=\"python\")\n",
    "business_df_replace = business_df.copy()\n",
    "# default value to replace for ambience when it is Nan\n",
    "ambience_default = str({'romantic': False, 'intimate': False, 'classy': False, 'hipster': False, 'divey': False, 'touristy': False, 'trendy': False, 'upscale': False, 'casual': False})\n",
    "business_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection\n",
    "\n",
    "I decided that we are only attempting to use features when more than half of its values are not Nan as otherwise, there are too less datapoints with a value. This percent non-Nan requirement can be changed nevertheless. \n",
    "\n",
    "I also decided to not look at the hours as it seems to complex to make into numerical value and would not help much in determining a user's review from intuition. The same is the case with the latitude, longitude, name, and address features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_df_replace.drop([col for col in business_df.columns if business_df[col].isnull().sum() > 0.5 * 12058 ], axis=1, inplace=True)\n",
    "business_df_replace.drop(['hours_Friday', 'hours_Monday', 'hours_Saturday',\n",
    "       'hours_Sunday', 'hours_Thursday', 'hours_Tuesday', 'hours_Wednesday',\n",
    "       'is_open', 'latitude', 'longitude', 'postal_code', 'name', 'address', 'categories'], axis=1, inplace=True)\n",
    "business_df_replace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Helper Functions\n",
    "\n",
    "##### view_column_values\n",
    "Helps to view what values occur inside the column of a dataframe\n",
    "\n",
    "##### expand_dict_to_columns\n",
    "Sometimes there are columns in the dataframe in which the data is a dictionary string(such as attributes_Ambience). This function helps expand that dictionary string into extra columns with the column being the key and the row content being the value. It returns the modified dataframe.\n",
    "\n",
    "##### replace_column_nan\n",
    "There are many Nan in the data. This function replaces the Nan of a specifc column of a dataframe with one of the values that already occur. The third parameter index_of_value_count is used to specify what value to replace, the values which can be viewed using view_column_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def view_column_values(df, column_name):\n",
    "    return df[column_name].value_counts()\n",
    "\n",
    "def expand_dict_to_columns(df, column_name):\n",
    "    expanded_df = df[column_name].apply(lambda x : dict(eval(x))).apply(pd.Series)\n",
    "    expanded_df.fillna(False, inplace=True)\n",
    "    df = pd.concat([df, expanded_df], axis = 1)\n",
    "    df.drop([column_name], axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "def replace_column_nan(df, column_name, index_of_value_count):\n",
    "    df[column_name] = df[column_name].fillna(df[column_name].value_counts().index[index_of_value_count])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_column_values(business_df, 'stars')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_df['stars'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replacing all the NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "business_df_replace['attributes_Ambience'] = business_df_replace['attributes_Ambience'].fillna(ambience_default)\n",
    "business_df_replace =expand_dict_to_columns(business_df_replace, 'attributes_Ambience')\n",
    "replace_column_nan(business_df_replace, 'attributes_Alcohol', 0)      # default full_bar, to change to none, change last parameter to 1 \n",
    "replace_column_nan(business_df_replace, 'attributes_BikeParking', 0)  # default yes parking\n",
    "replace_column_nan(business_df_replace, 'attributes_BusinessAcceptsCreditCards', 1)  # default True\n",
    "replace_column_nan(business_df_replace, 'attributes_BusinessParking', 0)  # default just lot parking\n",
    "business_df_replace = expand_dict_to_columns(business_df_replace, 'attributes_BusinessParking')\n",
    "replace_column_nan(business_df_replace, 'attributes_Caters', 0)  # default True\n",
    "replace_column_nan(business_df_replace, 'attributes_GoodForKids', 0)  # default True\n",
    "replace_column_nan(business_df_replace, 'attributes_HasTV', 0)  # default True\n",
    "replace_column_nan(business_df_replace, 'attributes_NoiseLevel', 0)  # default Average\n",
    "replace_column_nan(business_df_replace, 'attributes_OutdoorSeating', 0)  # default True\n",
    "replace_column_nan(business_df_replace, 'attributes_GoodForMeal', 0)  # default good for lunch and dinner\n",
    "business_df_replace = expand_dict_to_columns(business_df_replace, 'attributes_GoodForMeal')\n",
    "replace_column_nan(business_df_replace, 'attributes_RestaurantsAttire', 0)  # default casual\n",
    "replace_column_nan(business_df_replace, 'attributes_RestaurantsDelivery', 0)  # default false\n",
    "replace_column_nan(business_df_replace, 'attributes_RestaurantsGoodForGroups', 0)  # default true\n",
    "replace_column_nan(business_df_replace, 'attributes_RestaurantsPriceRange2', 0)  # default 2$ signs\n",
    "replace_column_nan(business_df_replace, 'attributes_RestaurantsReservations', 0)  # default true\n",
    "replace_column_nan(business_df_replace, 'attributes_RestaurantsTableService', 0)  # default true\n",
    "replace_column_nan(business_df_replace, 'attributes_RestaurantsTakeOut', 0)  # default true\n",
    "replace_column_nan(business_df_replace, 'attributes_WheelchairAccessible', 0)  # default true\n",
    "replace_column_nan(business_df_replace, 'attributes_WiFi', 0)  # default free\n",
    "replace_column_nan(business_df_replace, 'attributes_RestaurantsTableService', 0)  # default true\n",
    "replace_column_nan(business_df_replace, 'attributes_RestaurantsTableService', 0)  # default true\n",
    "replace_column_nan(business_df_replace, 'attributes_RestaurantsTableService', 0)  # default true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Changing categorical input to numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for col in business_df_replace.columns:\n",
    "    if col not in ['business_id', 'stars']: \n",
    "        if business_df_replace[col].dtypes == bool:\n",
    "            # true becomes 1, false becomes 0\n",
    "            business_df_replace[col] *=1\n",
    "        elif business_df_replace[col].dtypes != np.dtype('int32') and business_df_replace[col].dtypes != np.dtype('int64') and business_df_replace[col].dtypes != float: \n",
    "            #changes categorical values to numerical values\n",
    "            business_df_replace[col] = business_df_replace[col].astype('category').cat.codes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#note, the business_id column is not numerical, I kept it the same\n",
    "business_df_replace.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# USE THIS\n",
    "business_df_replace.drop(['attributes_BikeParking',\n",
    "       'attributes_BusinessAcceptsCreditCards', 'attributes_Caters',\n",
    "       'attributes_GoodForKids', 'attributes_HasTV', 'attributes_NoiseLevel',\n",
    "       'attributes_OutdoorSeating', 'attributes_RestaurantsAttire',\n",
    "       'attributes_RestaurantsDelivery', 'attributes_RestaurantsGoodForGroups',\n",
    "       'attributes_RestaurantsReservations',\n",
    "       'attributes_RestaurantsTakeOut','attributes_WiFi', 'garage', 'lot', 'street', 'valet',\n",
    "       'validated', 'state',\n",
    "#        'casual', 'classy', 'hipster', 'intimate', 'romantic', 'touristy',\n",
    "#        'trendy', 'upscale', 'divey', 'breakfast', 'brunch', 'dessert',\n",
    "#        'dinner', 'latenight', 'lunch',\n",
    "                          'attributes_RestaurantsTableService', 'attributes_WheelchairAccessible'], axis=1, inplace=True)\n",
    "business_df_replace.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing User data\n",
    "The users are mostly good for numerical features. However the following features 'elite', 'friends', 'name', 'yelping_since' are not but I decided to drop them as they do not intuitively seem super important. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df = pd.read_csv(\"all/users.csv\")\n",
    "users_df_replace = users_df.copy()\n",
    "users_df_replace.drop([ 'compliment_cool', 'compliment_cute', 'compliment_funny', 'compliment_hot', 'compliment_list', 'compliment_more',\n",
    "                       'compliment_note', 'compliment_photos', 'compliment_plain', 'compliment_profile', 'compliment_writer','name',\n",
    "                       'friends', 'fans',\n",
    "                       'yelping_since', \n",
    "                       'elite',\n",
    "                       'cool', 'funny',\n",
    "                      ], axis=1, inplace=True)\n",
    "# users_df_replace['elite'] = (users_df_replace['elite'] == True).astype(int)\n",
    "# users_df_replace['yelping_since'] = users_df_replace['yelping_since'].map(lambda x: pd.to_datetime(x).timestamp())\n",
    "users_df_replace.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#check that all data is numerical, should output nothing if it is\n",
    "for col in users_df_replace.columns:\n",
    "    if col not in ['user_id']: \n",
    "        if users_df_replace[col].dtypes != np.dtype('int32') and users_df_replace[col].dtypes != np.dtype('int64') and users_df_replace[col].dtypes != float:\n",
    "            print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check that there are no Nan values\n",
    "users_df_replace.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this\n",
    "users_df_replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import StandardScaler\n",
    "# sc = StandardScaler()\n",
    "# cols_to_norm = ['review_count', 'useful']\n",
    "# for col in cols_to_norm:\n",
    "#     users_df_replace[col] = sc.fit_transform(users_df_replace[col])\n",
    "# users = sc.fit_transform(users_df_replace)\n",
    "# cols = users_df_replace.columns\n",
    "# ids = users_df_replace['user_id']\n",
    "# avg_stars = users_df_replace['average_stars']\n",
    "# users = users_df_replace.copy()\n",
    "# users.drop(['average_stars', 'user_id'], axis=1, inplace=True)\n",
    "\n",
    "# users_clean = pd.DataFrame(columns=cols)\n",
    "# users_df_replace['review_count']\n",
    "# users_df_replace[cols_to_norm] = sc.fit_transform(users_df_replace[cols_to_norm])\n",
    "# users_df_replace\n",
    "# sc.fit_transform([users_df_replace['review_count']])\n",
    "# users_df_replace['review_count']\n",
    "# y_train = sc.fit_transform(test_df_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Review Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For review Data we will use the user id and buisiness id as reference and do a join with our business and user dataframes. This will the basis of our training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(\"all/sample_submission.csv\")\n",
    "train_reviews = pd.read_csv(\"all/train_reviews.csv\")\n",
    "train_reviews = train_reviews[['user_id', 'business_id', 'stars']]\n",
    "train_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_reviews_replace = train_reviews.copy()\n",
    "reviews_denorm = pd.merge(train_reviews_replace, users_df_replace, how='left', on='user_id')\n",
    "# reviews_denorm\n",
    "reviews_denorm = pd.merge(reviews_denorm, business_df_replace, how='inner', on='business_id')\n",
    "business_df_replace.columns\n",
    "reviews_denorm = reviews_denorm.rename(columns={'stars_x': 'review_stars', 'stars_y': 'business_stars'})\n",
    "train_df_y = reviews_denorm['review_stars']\n",
    "train_df_x = reviews_denorm.copy()\n",
    "train_df_x.drop(['review_stars', 'business_id', 'user_id'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_denorm.groupby('review_stars').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#validate df will be our validation set\n",
    "validate_df = pd.read_csv(\"all/validate_queries.csv\")\n",
    "validate_df_denorm = pd.merge(validate_df, users_df_replace, how='left', on='user_id')\n",
    "validate_df_denorm = pd.merge(validate_df_denorm, business_df_replace, how='left', on='business_id')\n",
    "validate_df_denorm = validate_df_denorm.rename(columns={'stars_x': 'review_stars', 'stars_y': 'business_stars'})\n",
    "test_df_y = validate_df_denorm['review_stars']\n",
    "test_df_x = validate_df_denorm.copy()\n",
    "test_df_x.drop(['Unnamed: 0', 'review_stars', 'business_id', 'user_id'], axis=1, inplace=True)\n",
    "\n",
    "# train_df_x = pd.concat([train_df_x, test_df_x])\n",
    "# train_df_y = pd.concat([train_df_y, test_df_y])\n",
    "# test_df_x.columns, train_df_x.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usefull should be normalized by the number of reviews a user has given\n",
    "train_df_x['useful'] = train_df_x['useful'] / train_df_x['review_count_x']\n",
    "test_df_x['useful'] = test_df_x['useful'] / test_df_x['review_count_x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "#z score normalize our inputs to our model \n",
    "scaler = StandardScaler()\n",
    "replace_column_nan(train_df_x, 'useful', 0) \n",
    "scaler.fit(train_df_x)\n",
    "replace_column_nan(train_df_x, 'useful', 0) \n",
    "train_x = scaler.transform(train_df_x)\n",
    "replace_column_nan(test_df_x, 'useful', 0) \n",
    "scaler.fit(test_df_x)\n",
    "test_x = scaler.transform(test_df_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, make_scorer, mean_squared_error\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "# for max_itr in [200, 500, 700]:\n",
    "#     for alph in [1e-4, 1e-3, 1e-2]:\n",
    "#         reviews_denorm.shape\n",
    "#         clf = MLPClassifier(solver='sgd', alpha=alph, hidden_layer_sizes=(100, 100, 8), learning_rate='adaptive', learning_rate_init= 0.0001, max_iter=max_itr)\n",
    "#         clf.fit(train_df_x, train_df_y.values)      \n",
    "#         y_pred = clf.predict(test_df_x)\n",
    "\n",
    "# clf = MLPRegressor(solver='sgd', alpha=1e-4, hidden_layer_sizes=(50, 50, 8), learning_rate='adaptive', learning_rate_init= 0.0001, max_iter=200)\n",
    "# clf.fit(train_df_x, train_df_y.values)      \n",
    "# y_pred = clf.predict(test_df_x)\n",
    "# y_pred_train = clf.predict(train_df_x)\n",
    "\n",
    "for max_itr in [200, 500, 700, 1000]:\n",
    "    for alph in [1e-4, 1e-3, 5e-3, 1e-2]:\n",
    "        for hidden_layer_size in [25, 50, 100]:\n",
    "            print(\"Reporting Stats: max_iter: {}, alpha: {}, hidden_layer_size: {}\".format(max_itr, alph, hidden_layer_size))\n",
    "            network_topography = (hidden_layer_size, hidden_layer_size, int(hidden_layer_size/5))\n",
    "            clf = MLPRegressor(solver='sgd', alpha=alph, hidden_layer_sizes=network_topography, learning_rate='adaptive', learning_rate_init=0.0001, max_iter=max_itr)\n",
    "            clf.fit(train_x, train_df_y.values) \n",
    "            y_pred_train = clf.predict(train_df_x).round(decimals=0)\n",
    "            y_pred = clf.predict(test_x).round(decimals=0)\n",
    "            print(\"training_report\")\n",
    "            report_mse_accuracy(train_df_y.values, y_pred_train)\n",
    "            print(\"validation_report\")\n",
    "            report_mse_accuracy(test_df_y.values, y_pred)\n",
    "            print(\"\\n\\n\")\n",
    "# print(classification_report(test_df_))\n",
    "def report_mse_accuracy(y_true, y_pred):\n",
    "    #print(classification_report(y_true, y_pred)) # print classification report\n",
    "    print(\"Validation MSE {}\".format(mean_squared_error(y_true, y_pred)))\n",
    "    print(\"Accuracy{}\".format(y_true, y_pred))\n",
    "    return accuracy_score(y_true, y_pred) # return accuracy score\n",
    "\n",
    "\n",
    "# for k in [15, 25, 40, 55, 80, 125, 250, 350, 500, 1000]:\n",
    "#     classifier = KNeighborsClassifier(n_neighbors=k, algorithm='auto', weights='distance') \n",
    "#     classifier.fit(train_df_x.values, train_df_y.values)\n",
    "#     scores = cross_val_score(classifier, X=train_df_x, y=train_df_y, cv=5, \\\n",
    "#                scoring=make_scorer(classification_report_with_accuracy_score))\n",
    "#     print(k, scores)\n",
    "#     y_pred = classifier.predict(test_df_x)\n",
    "#     print(k, classification_report(test_df_y.values, y_pred))\n",
    "\n",
    "# train_df_x.drop(['yelping_since'], axis=1, inplace=True)\n",
    "# train_df_x\n",
    "\n",
    "# for depth in (7, 10, 15):\n",
    "# print(7)\n",
    "# regressor = RandomForestClassifier(n_estimators=150, max_depth=7, min_samples_split=5)\n",
    "# regressor.fit(train_df_x, train_df_y)\n",
    "# scores = cross_val_score(regressor, X=train_df_x, y=train_df_y, cv=5, \\\n",
    "#            scoring=make_scorer(classification_report_with_accuracy_score))\n",
    "# print(10)\n",
    "# regressor = RandomForestClassifier(n_estimators=150, max_depth=10, min_samples_split=5)\n",
    "# regressor.fit(train_df_x, train_df_y)\n",
    "# scores = cross_val_score(regressor, X=train_df_x, y=train_df_y, cv=5, \\\n",
    "#            scoring=make_scorer(classification_report_with_accuracy_score))\n",
    "# print(15)\n",
    "# regressor = RandomForestClassifier(n_estimators=150, max_depth=15, min_samples_split=5)\n",
    "# regressor.fit(train_df_x, train_df_y)\n",
    "# scores = cross_val_score(regressor, X=train_df_x, y=train_df_y, cv=5, \\\n",
    "#            scoring=make_scorer(classification_report_with_accuracy_score))\n",
    "#     print(depth, scores)\n",
    "\n",
    "#y_pred = regressor.predict(test_df_x.values)\n",
    "\n",
    "# print(classification_report(test_df_y.values, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mean_squared_error(test_df_y.values, y_pred.round(decimals=0))\n",
    "y_pred_train = clf.predict(train_df_x)\n",
    "mean_squared_error(train_df_y.values, y_pred_train.round(decimals=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred.round(decimals=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_df_denorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.Series(y_pred)\n",
    "df.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"all/test_queries.csv\")\n",
    "test = pd.merge(test, users_df_replace, how='left', on='user_id')\n",
    "test = pd.merge(test, business_df_replace, how='left', on='business_id')\n",
    "\n",
    "test = test.rename(columns={'stars': 'business_stars'})\n",
    "# test.columns\n",
    "# test_x = test.copy()\n",
    "test.drop(['business_id', 'user_id'], axis=1, inplace=True)\n",
    "submit_y = regressor.predict(test.values)\n",
    "# submit_y = pd.DataFrame(submit_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Run to put in submit format\n",
    "\n",
    "submit = pd.DataFrame(columns=['stars'])\n",
    "submit['stars'] = submit_y\n",
    "\n",
    "submit.index.name = 'index'\n",
    "submit.to_csv('submit.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"all/test_queries.csv\")\n",
    "test = pd.merge(test, users_df_replace, how='left', on='user_id')\n",
    "test = pd.merge(test, business_df_replace, how='left', on='business_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
